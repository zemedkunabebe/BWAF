{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e96b58f-c437-443d-a492-a6e0db4e22da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_3416\\2615610857.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined losses CSV to:\n",
      "E:\\results_msBERT_replication\\all_kmer_losses.csv\n",
      "       kmer_model  epoch  train_loss  val_loss\n",
      "0  checkpoints_k3      0    0.569749  0.509271\n",
      "1  checkpoints_k3      1    0.511882  0.501003\n",
      "2  checkpoints_k3      2    0.503876  0.484983\n",
      "3  checkpoints_k3      3    0.498610  0.483134\n",
      "4  checkpoints_k3      4    0.493705  0.478425\n",
      "5  checkpoints_k3      5    0.491025  0.474061\n",
      "6  checkpoints_k3      6    0.488256  0.512219\n",
      "7  checkpoints_k3      7    0.483946  0.469574\n",
      "8  checkpoints_k3      8    0.482185  0.485143\n",
      "9  checkpoints_k3      9    0.478302  0.496562\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Base directory containing your kmer checkpoints folders\n",
    "base_dir = r\"E:\\results_msBERT_replication\"\n",
    "\n",
    "# The kmer folders to process\n",
    "kmer_folders = ['checkpoints_k3', 'checkpoints_k4', 'checkpoints_k5', 'checkpoints_k6']\n",
    "\n",
    "# Prepare data collection\n",
    "data = []\n",
    "\n",
    "for kmer in kmer_folders:\n",
    "    folder_path = os.path.join(base_dir, kmer)\n",
    "    \n",
    "    # List checkpoint files sorted by epoch number\n",
    "    checkpoints = sorted(\n",
    "        [f for f in os.listdir(folder_path) if f.startswith('checkpoint_epoch_') and f.endswith('.pth')],\n",
    "        key=lambda x: int(x.split('_')[-1].split('.')[0])  # extract epoch number for sorting\n",
    "    )\n",
    "    \n",
    "    for ckpt_file in checkpoints:\n",
    "        epoch_num = int(ckpt_file.split('_')[-1].split('.')[0])\n",
    "        ckpt_path = os.path.join(folder_path, ckpt_file)\n",
    "        \n",
    "        checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "        \n",
    "        # Extract train and val losses lists (assuming they exist and are lists)\n",
    "        train_losses = checkpoint.get('train_losses', [])\n",
    "        val_losses = checkpoint.get('val_losses', [])\n",
    "        \n",
    "        # Sometimes train_losses and val_losses might be stored as lists per epoch or just scalars\n",
    "        # We want the loss of this epoch:\n",
    "        # Let's try to get the last element if list, or the scalar directly\n",
    "        \n",
    "        train_loss = train_losses[epoch_num] if isinstance(train_losses, list) and len(train_losses) > epoch_num else (train_losses if isinstance(train_losses, float) else None)\n",
    "        val_loss = val_losses[epoch_num] if isinstance(val_losses, list) and len(val_losses) > epoch_num else (val_losses if isinstance(val_losses, float) else None)\n",
    "        \n",
    "        # Append to data list\n",
    "        data.append({\n",
    "            'kmer_model': kmer,\n",
    "            'epoch': epoch_num,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sort nicely by model and epoch\n",
    "df = df.sort_values(['kmer_model', 'epoch']).reset_index(drop=True)\n",
    "\n",
    "# Save as CSV\n",
    "output_csv = os.path.join(base_dir, 'all_kmer_losses.csv')\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Saved combined losses CSV to:\\n{output_csv}\")\n",
    "\n",
    "# Print some rows to check\n",
    "print(df.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
