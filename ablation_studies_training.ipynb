{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f709ba9-f066-4e6e-ad05-376e4ed8a504",
   "metadata": {},
   "source": [
    "# **Ablation Studies for BWAF Model Validation**\n",
    "\n",
    "**Objective:**\n",
    "This notebook implements and evaluates several ablated versions of our BWAF model to systematically dissect the sources of its performance and validate our architectural choices. The goal is to quantify the contribution of each data modality (Sequence, Network, Priors) and the novel BWAF fusion mechanism itself.\n",
    "\n",
    "**All models will be trained and evaluated on the identical data splits** derived from the main experimental pipeline to ensure a fair and direct comparison.\n",
    "\n",
    "**Ablation Models to be Tested:**\n",
    "1.  **Ablation 1 (A1): Transformer + Priors Only** (No GAT branch)\n",
    "2.  **Ablation 2 (A2): GAT + Priors Only** (No Transformer branch)\n",
    "3.  **Ablation 3 (A3): Simple Concatenation Fusion** (Transformer + GAT + Priors, no BWAF weighting)\n",
    "4.  **Ablation 4 (A4): Transformer Only** (No GAT or Prior data)\n",
    "5.  **Ablation 5 (A5): Priors Only** (Simple MLP on motif counts)\n",
    "\n",
    "---\n",
    "## **0. Setup and Imports**\n",
    "This cell imports all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8676f9ab-fefc-4871-9361-994646ce98b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful.\n",
      "PyTorch Version: 2.7.0+cu126\n"
     ]
    }
   ],
   "source": [
    "# %% 0. Setup and Imports\n",
    "# ============================================================================\n",
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import gzip\n",
    "import time\n",
    "import argparse\n",
    "import datetime\n",
    "import sys\n",
    "import warnings\n",
    "import random\n",
    "import traceback\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                           f1_score, roc_auc_score, confusion_matrix)\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch Geometric imports (needed for GAT-related ablations)\n",
    "try:\n",
    "    from torch_geometric.nn import GATConv\n",
    "    from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "except ImportError:\n",
    "    print(\"PyTorch Geometric might not be fully installed. GAT-based ablations will fail.\")\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "print(\"Imports successful.\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c26001-92e5-43b8-bca6-d71bd0abe938",
   "metadata": {},
   "source": [
    "## **1. Configuration / Constants**\n",
    "Defines paths and hyperparameters. These settings are kept consistent with the main BWAF experiment for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "615578db-1793-4c07-be81-23c58e3a3499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation/Baseline Runs: Using device: cpu\n",
      "Ablation/Baseline Output directory: results_ablation_studies/\n"
     ]
    }
   ],
   "source": [
    "# %% 1. Configuration / Constants\n",
    "# ============================================================================\n",
    "# --- Data Paths (Must be the same as the main BWAF experiment) ---\n",
    "BASE_DATA_DIR = './data/'\n",
    "SEQ_DATA_DIR = os.path.join(BASE_DATA_DIR, 'raw/human_genome_annotation')\n",
    "PRIOR_DATA_DIR = os.path.join(BASE_DATA_DIR, 'raw/human_genome_annotation')\n",
    "GAT_PREPROCESSED_DIR = os.path.join(BASE_DATA_DIR, 'preprocessed/gat_normalized') # Pre-aligned and normalized GAT data\n",
    "\n",
    "PROMOTER_SEQ_FILE = os.path.join(SEQ_DATA_DIR, 'updated_promoter_features_clean.csv')\n",
    "NON_PROMOTER_SEQ_FILE = os.path.join(SEQ_DATA_DIR, 'updated_non_promoter_sequences.csv')\n",
    "PRIOR_FILE = os.path.join(PRIOR_DATA_DIR, 'biological_prior_for_transformer_branch.csv')\n",
    "\n",
    "# --- Model Hyperparameters (Consistent with BWAF model) ---\n",
    "SEQ_LEN = 2000\n",
    "PAD_IDX = 4\n",
    "VOCAB_SIZE = 5\n",
    "EMBEDDING_DIM = 64\n",
    "NUM_ATTN_HEADS = 4\n",
    "NUM_TRANSFORMER_LAYERS = 2\n",
    "TRANSFORMER_FF_DIM = EMBEDDING_DIM * 4\n",
    "GAT_OUTPUT_DIM = 64\n",
    "FUSION_HIDDEN_DIM = 128\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 16 \n",
    "NUM_EPOCHS = 10 # Same number of epochs as main run for comparison\n",
    "VALIDATION_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15\n",
    "RANDOM_SEED = 42\n",
    "OPTIMIZER_WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# --- Output Files ---\n",
    "OUTPUT_DIR_ABLATION = 'results_ablation_studies/'\n",
    "os.makedirs(OUTPUT_DIR_ABLATION, exist_ok=True)\n",
    "LOG_FILE_ABLATION = os.path.join(OUTPUT_DIR_ABLATION, 'ablation_log.txt')\n",
    "\n",
    "# --- Hardware ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Setup ---\n",
    "random.seed(RANDOM_SEED); np.random.seed(RANDOM_SEED); torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(f\"Ablation/Baseline Runs: Using device: {DEVICE}\")\n",
    "print(f\"Ablation/Baseline Output directory: {OUTPUT_DIR_ABLATION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3dc22-dba9-435a-b627-8b5da33563a6",
   "metadata": {},
   "source": [
    "## **2. Utility Functions & Data Loading**\n",
    "Reusing utility functions for logging, encoding, and data loading from the main script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0413b12-30bd-41e0-a83b-a153884ab4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-09 18:58:11] Log file: results_ablation_studies/ablation_log.txt\n"
     ]
    }
   ],
   "source": [
    "# %% 2. Utility Functions & Data Loading\n",
    "# ============================================================================\n",
    "def log_message(message, log_file=LOG_FILE_ABLATION):\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    full_message = f\"[{timestamp}] {message}\"\n",
    "    print(full_message)\n",
    "    log_dir = os.path.dirname(log_file);\n",
    "    if log_dir and not os.path.exists(log_dir): os.makedirs(log_dir)\n",
    "    with open(log_file, 'a', encoding='utf-8') as f: f.write(full_message + '\\n')\n",
    "\n",
    "with open(LOG_FILE_ABLATION, 'w', encoding='utf-8') as f: f.write(f\"--- Ablation Log Initialized ---\\n\")\n",
    "log_message(f\"Log file: {LOG_FILE_ABLATION}\")\n",
    "\n",
    "def integer_encode_sequence(sequence, max_len=SEQ_LEN):\n",
    "    encoding_map={'A':0,'T':1,'C':2,'G':3}; encoded=np.full(max_len,PAD_IDX,dtype=np.int64)\n",
    "    for i,nuc in enumerate(sequence[:max_len]): encoded[i]=encoding_map.get(nuc.upper(), PAD_IDX)\n",
    "    return encoded\n",
    "def log_transform_priors(counts): return np.log1p(np.array(counts,dtype=np.float32))\n",
    "def extract_clean_gene_id(series): return series.astype(str).str.extract(r'(ENSG\\d+)', expand=False).fillna('UNKNOWN')\n",
    "\n",
    "def load_sequences(file_path, is_promoter=True):\n",
    "    log_message(f\"Loading sequences from {file_path}...\")\n",
    "    df = pd.read_csv(file_path); seq_col='promoter_sequence' if 'promoter_sequence' in df.columns else 'sequence'\n",
    "    df = df[df[seq_col].str.len() == SEQ_LEN]; df = df[~df[seq_col].str.contains('N', na=False, case=False)]\n",
    "    return df[seq_col].tolist(), extract_clean_gene_id(df['gene_id']).tolist(), [1 if is_promoter else 0]*len(df)\n",
    "\n",
    "def load_priors(file_path, gene_id_order):\n",
    "    log_message(f\"Loading priors from {file_path}...\")\n",
    "    df=pd.read_csv(file_path); df['clean_gene_id']=extract_clean_gene_id(df['gene_id'])\n",
    "    count_cols=[c for c in df.columns if '(Count)' in c]; prior_dim=len(count_cols)\n",
    "    df_priors = df[['clean_gene_id']+count_cols].copy(); df_priors.set_index('clean_gene_id', inplace=True)\n",
    "    df_priors=df_priors[~df_priors.index.duplicated(keep='first')]; aligned=df_priors.reindex(gene_id_order, fill_value=0)\n",
    "    return log_transform_priors(aligned.values), prior_dim\n",
    "\n",
    "class MainDataset(Dataset):\n",
    "    def __init__(self, sequences, gene_ids_for_samples, labels, biological_priors, precomputed_gat_output, gat_gene_order):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.int64)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)\n",
    "        self.priors = torch.tensor(biological_priors, dtype=torch.float32)\n",
    "        self.precomputed_gat_output = torch.tensor(precomputed_gat_output, dtype=torch.float32)\n",
    "        gat_gene_to_idx = {gene: idx for idx, gene in enumerate(gat_gene_order)}\n",
    "        self.sample_id_to_gat_idx = np.array([gat_gene_to_idx.get(gid, -1) for gid in gene_ids_for_samples], dtype=np.int64)\n",
    "    def __len__(self): return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        gat_idx = self.sample_id_to_gat_idx[idx]\n",
    "        graph_features = self.precomputed_gat_output[gat_idx] if gat_idx != -1 else torch.zeros(self.precomputed_gat_output.shape[1])\n",
    "        return {'sequence': self.sequences[idx], 'priors': self.priors[idx], 'graph_features': graph_features, 'label': self.labels[idx]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b2e121-19ef-49b0-9802-c61dabc6dfb3",
   "metadata": {},
   "source": [
    "## **3. Model Definitions for Ablation Studies**\n",
    "Here we define the architectures for our comparative experiments. We reuse the `TransformerBranch` from the main project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19ef111a-2381-4c6a-a2e2-9572bf9e4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 3. Model Definitions for Ablation Studies\n",
    "# ============================================================================\n",
    "class TransformerBranch(nn.Module):\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, seq_len=SEQ_LEN, embed_dim=EMBEDDING_DIM,\n",
    "                 num_heads=NUM_ATTN_HEADS, ff_dim=TRANSFORMER_FF_DIM,\n",
    "                 num_layers=NUM_TRANSFORMER_LAYERS, dropout=DROPOUT_RATE):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, seq_len, embed_dim))\n",
    "        self.embed_dropout = nn.Dropout(p=dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim,\n",
    "            dropout=dropout, activation='relu', batch_first=True, norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fusion_prep_layer = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, embed_dim), nn.ReLU(), nn.Dropout(p=dropout))\n",
    "    def forward(self, seq_data):\n",
    "        x = self.embedding(seq_data) + self.positional_encoding[:, :seq_data.size(1), :]\n",
    "        x = self.embed_dropout(x); padding_mask = (seq_data == PAD_IDX)\n",
    "        transformer_output = self.transformer_encoder(x, src_key_padding_mask=padding_mask)\n",
    "        mask = (~padding_mask).unsqueeze(-1).float()\n",
    "        aggregated_output = (transformer_output * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1.0)\n",
    "        return self.fusion_prep_layer(aggregated_output)\n",
    "\n",
    "# --- Ablation 1 (A1): Transformer + Priors Only ---\n",
    "class TransformerPriorsOnly(nn.Module):\n",
    "    def __init__(self, prior_dim, embed_dim, fusion_hidden_dim, dropout):\n",
    "        super().__init__(); self.transformer = TransformerBranch(embed_dim=embed_dim, dropout=dropout)\n",
    "        combined_dim = embed_dim + prior_dim\n",
    "        self.classifier=nn.Sequential(nn.LayerNorm(combined_dim), nn.Linear(combined_dim, fusion_hidden_dim),\n",
    "                                      nn.ReLU(), nn.Dropout(dropout), nn.Linear(fusion_hidden_dim, 1))\n",
    "    def forward(self, batch):\n",
    "        seq_features = self.transformer(batch['sequence'])\n",
    "        combined = torch.cat([seq_features, batch['priors']], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "# --- Ablation 2 (A2): GAT + Priors Only ---\n",
    "class GATPriorsOnly(nn.Module):\n",
    "    def __init__(self, prior_dim, gat_output_dim, fusion_hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        combined_dim = gat_output_dim + prior_dim\n",
    "        self.classifier=nn.Sequential(nn.LayerNorm(combined_dim), nn.Linear(combined_dim, fusion_hidden_dim),\n",
    "                                      nn.ReLU(), nn.Dropout(dropout), nn.Linear(fusion_hidden_dim, 1))\n",
    "    def forward(self, batch):\n",
    "        combined = torch.cat([batch['graph_features'], batch['priors']], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "# --- Ablation 3 (A3): Simple Concatenation Fusion ---\n",
    "class SimpleConcatModel(nn.Module):\n",
    "    def __init__(self, prior_dim, embed_dim, gat_output_dim, fusion_hidden_dim, dropout):\n",
    "        super().__init__(); self.transformer = TransformerBranch(embed_dim=embed_dim, dropout=dropout)\n",
    "        combined_dim = embed_dim + gat_output_dim + prior_dim\n",
    "        self.classifier=nn.Sequential(nn.LayerNorm(combined_dim), nn.Linear(combined_dim, fusion_hidden_dim),\n",
    "                                      nn.ReLU(), nn.Dropout(dropout), nn.Linear(fusion_hidden_dim, 1))\n",
    "    def forward(self, batch):\n",
    "        seq_features = self.transformer(batch['sequence'])\n",
    "        combined = torch.cat([seq_features, batch['graph_features'], batch['priors']], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "# --- Ablation 4 (A4): Transformer Only ---\n",
    "class TransformerOnly(nn.Module):\n",
    "    def __init__(self, embed_dim, fusion_hidden_dim, dropout):\n",
    "        super().__init__(); self.transformer = TransformerBranch(embed_dim=embed_dim, dropout=dropout)\n",
    "        self.classifier = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, fusion_hidden_dim),\n",
    "                                      nn.ReLU(), nn.Dropout(dropout), nn.Linear(fusion_hidden_dim, 1))\n",
    "    def forward(self, batch):\n",
    "        seq_features = self.transformer(batch['sequence'])\n",
    "        return self.classifier(seq_features)\n",
    "\n",
    "# --- Ablation 5 (A5): Priors Only ---\n",
    "class PriorsOnly(nn.Module):\n",
    "    def __init__(self, prior_dim, fusion_hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(prior_dim),\n",
    "            nn.Linear(prior_dim, fusion_hidden_dim), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(fusion_hidden_dim, fusion_hidden_dim // 2), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(fusion_hidden_dim // 2, 1))\n",
    "    def forward(self, batch):\n",
    "        return self.classifier(batch['priors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4595c5f2-ec75-4ae9-8e9d-18ff1874da2f",
   "metadata": {},
   "source": [
    "## **4. Resumable Experiment Execution Framework**\n",
    "This generic `run_experiment` function handles the training and evaluation for any given model, now with robust checkpointing to resume training if interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c6d93b5-8e47-4540-8fb6-8d3406566774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 4. Experiment Execution Framework (Resumable, with Duration Logging)\n",
    "# ============================================================================\n",
    "\n",
    "def run_experiment(model_name, model_instance, train_loader, val_loader, test_loader,\n",
    "                   num_epochs, learning_rate, device, output_dir,\n",
    "                   precomputed_gat_output=None):\n",
    "    \"\"\"\n",
    "    A generic, resumable function to train and evaluate a given model instance for ablation studies.\n",
    "    Includes per-epoch duration logging and robust checkpointing.\n",
    "    \"\"\"\n",
    "    log_message(f\"\\n{'='*25} STARTING EXPERIMENT: {model_name} {'='*25}\")\n",
    "\n",
    "    # --- Setup paths for this specific experiment ---\n",
    "    exp_output_dir = os.path.join(output_dir, model_name)\n",
    "    os.makedirs(exp_output_dir, exist_ok=True)\n",
    "    checkpoint_dir = os.path.join(exp_output_dir, \"checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    model_best_save_path = os.path.join(exp_output_dir, f\"best_model_{model_name}.pth\")\n",
    "    loss_plot_path = os.path.join(exp_output_dir, f\"loss_plot_{model_name}.png\")\n",
    "    \n",
    "    # --- Initialize Model, Optimizer, Scheduler ---\n",
    "    model_instance.to(device)\n",
    "    \n",
    "    if hasattr(model_instance, 'precomputed_gat_output'):\n",
    "        model_instance.precomputed_gat_output = precomputed_gat_output\n",
    "        if precomputed_gat_output is not None and precomputed_gat_output.numel() > 0:\n",
    "             log_message(f\"Provided precomputed GAT output tensor to {model_name}.\")\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model_instance.parameters(), lr=learning_rate, weight_decay=OPTIMIZER_WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=3)\n",
    "\n",
    "    # --- RESUME FROM CHECKPOINT LOGIC ---\n",
    "    start_epoch = 0; train_losses, val_losses = [], []; best_val_loss = float('inf')\n",
    "    checkpoints = sorted(glob.glob(os.path.join(checkpoint_dir, \"checkpoint_epoch_*.pth\")),\n",
    "                         key=lambda x: int(re.search(r\"epoch_(\\d+)\",x).group(1)) if re.search(r\"epoch_(\\d+)\",x) else -1,\n",
    "                         reverse=True)\n",
    "    if checkpoints:\n",
    "        latest_checkpoint_path = checkpoints[0]\n",
    "        log_message(f\"Resuming {model_name} from checkpoint: {os.path.basename(latest_checkpoint_path)}\")\n",
    "        try:\n",
    "            checkpoint = torch.load(latest_checkpoint_path, map_location=device)\n",
    "            model_instance.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            if scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict'] is not None:\n",
    "                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            train_losses = checkpoint.get('train_losses',[]); val_losses = checkpoint.get('val_losses',[])\n",
    "            best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "            log_message(f\"Resumed from epoch {start_epoch-1}. Best Val Loss so far: {best_val_loss:.4f}\")\n",
    "        except Exception as e:\n",
    "            log_message(f\"Error loading checkpoint for {model_name}: {e}. Training from scratch.\")\n",
    "            start_epoch = 0\n",
    "\n",
    "    if start_epoch >= num_epochs:\n",
    "        log_message(f\"{model_name} training already completed. Loading best model for evaluation.\")\n",
    "        if os.path.exists(model_best_save_path):\n",
    "            model_instance.load_state_dict(torch.load(model_best_save_path, map_location=device))\n",
    "    else:\n",
    "        # --- TRAINING LOOP ---\n",
    "        log_message(f\"--- Training {model_name} from epoch {start_epoch} to {num_epochs-1} ---\")\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            model_instance.train(); running_loss=0.0\n",
    "            train_loop = tqdm(train_loader, desc=f\"{model_name} E{epoch+1} [Train]\", leave=False)\n",
    "            for batch in train_loop:\n",
    "                batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "                optimizer.zero_grad(); logits = model_instance(batch); loss = criterion(logits, batch['label'])\n",
    "                loss.backward(); optimizer.step(); running_loss += loss.item()\n",
    "                train_loop.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "            \n",
    "            avg_train_loss = running_loss/len(train_loader) if len(train_loader)>0 else 0.0\n",
    "            train_losses.append(avg_train_loss)\n",
    "\n",
    "            model_instance.eval(); val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    batch = {k:v.to(device) for k,v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "                    logits = model_instance(batch); val_loss += criterion(logits, batch['label']).item()\n",
    "            avg_val_loss = val_loss/len(val_loader) if len(val_loader)>0 else 0.0\n",
    "            val_losses.append(avg_val_loss)\n",
    "            \n",
    "            epoch_duration = time.time() - epoch_start_time; current_lr = optimizer.param_groups[0]['lr']\n",
    "            log_message(f\"{model_name} E{epoch+1}/{num_epochs}: TrL={avg_train_loss:.4f}, VaL={avg_val_loss:.4f}, Dur={epoch_duration:.2f}s, LR={current_lr:.2e}\")\n",
    "            if scheduler: scheduler.step(avg_val_loss)\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss; torch.save(model_instance.state_dict(), model_best_save_path)\n",
    "                log_message(f\"Saved new best {model_name} model.\")\n",
    "            \n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch:03d}.pth\")\n",
    "            torch.save({'epoch': epoch, 'model_state_dict': model_instance.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                        'train_losses': train_losses, 'val_losses': val_losses, 'best_val_loss': best_val_loss}, checkpoint_path)\n",
    "\n",
    "        if train_losses:\n",
    "            epochs_plotted = len(train_losses); epochs_range = range(1, epochs_plotted + 1)\n",
    "            plt.figure(figsize=(10,6)); plt.plot(epochs_range,train_losses,label='Train'); plt.plot(epochs_range,val_losses,label='Val');\n",
    "            plt.title(f'{model_name} Training Loss'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True); plt.savefig(loss_plot_path, dpi=300); plt.close()\n",
    "\n",
    "    # --- EVALUATION ---\n",
    "    log_message(f\"--- Evaluating best {model_name} model on Test Set ---\")\n",
    "    if os.path.exists(model_best_save_path): model_instance.load_state_dict(torch.load(model_best_save_path, map_location=device))\n",
    "    else: log_message(f\"Warning: Best model for {model_name} not found. Evaluating last epoch model.\")\n",
    "    \n",
    "    model_instance.eval(); all_labels, all_probs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f\"Eval {model_name}\", leave=False):\n",
    "            batch = {k:v.to(device) for k,v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            logits = model_instance(batch)\n",
    "            all_labels.extend(batch['label'].cpu().numpy()); all_probs.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "    \n",
    "    all_labels=np.array(all_labels).flatten(); all_probs=np.array(all_probs).flatten()\n",
    "    all_preds=(all_probs > 0.5).astype(int);\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds); f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0); rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=[0,1] if len(np.unique(all_labels))>=2 else np.unique(all_labels))\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.shape == (2,2) else (cm[0,0],0,0,0) if cm.shape==(1,1) and all_labels[0]==0 else (0,0,0,cm[0,0]) if cm.shape==(1,1) and all_labels[0]==1 else (0,0,0,0)\n",
    "    spec = tn/(tn+fp) if (tn+fp)>0 else 0.0\n",
    "    auc = roc_auc_score(all_labels, all_probs) if len(np.unique(all_labels))>1 else np.nan\n",
    "\n",
    "   \n",
    "    results = {'model': model_name, 'accuracy': acc, 'f1_score': f1, 'auc': auc, 'precision': prec, 'recall': rec, 'specificity': spec}\n",
    "    log_message(f\"--- {model_name} Test Results: ACC={acc:.4f}, F1={f1:.4f}, AUC={auc if not np.isnan(auc) else -1:.4f} ---\")\n",
    "    \n",
    "    results_path = os.path.join(exp_output_dir, f\"results_{model_name}.csv\")\n",
    "    pd.DataFrame([results]).to_csv(results_path, index=False)\n",
    "    log_message(f\"Saved results for {model_name} to {results_path}\")\n",
    "    \n",
    "    if 'plot_confusion_matrix' in globals():\n",
    "        cm_path = os.path.join(exp_output_dir, f\"cm_{model_name}.png\")\n",
    "        plot_confusion_matrix(cm, ['Non-P','P'], save_path=cm_path, title=f'{model_name} Confusion Matrix')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fa6f5f-1039-4dcf-ba2c-f60c4909ba25",
   "metadata": {},
   "source": [
    "## **5. Main Execution Block for Ablation & Baselines**\n",
    "This block sets up and runs each experiment sequentially, checking for completion before starting to allow for easy resumption of the entire experimental suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c28a4fd-d789-4c14-8a46-06dca83661e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-14 07:32:08] --- Starting Ablation & Baseline Experiment Workflow ---\n",
      "[2025-07-14 07:32:08] \n",
      "--- Step 1: Loading and Preparing Data for All Experiments ---\n",
      "[2025-07-14 07:32:08] Loading sequences from ./data/raw/human_genome_annotation/updated_promoter_features_clean.csv...\n",
      "[2025-07-14 07:32:09] Loading sequences from ./data/raw/human_genome_annotation/updated_non_promoter_sequences.csv...\n",
      "[2025-07-14 07:32:10] Loading priors from ./data/raw/human_genome_annotation/biological_prior_for_transformer_branch.csv...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc728180d3e54dd494d4416dfa7c432c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding All Seqs:   0%|          | 0/40056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-14 07:32:26] WARNING: Precomputed GAT output not found at results_bwaf_final/precomputed_gat_output.pt. GAT-related ablations (A2, A3) will use a zero tensor.\n",
      "[2025-07-14 07:32:26] Ablation Data Split: Train=28040, Val=6008, Test=6008\n",
      "[2025-07-14 07:32:26] \n",
      "========================= STARTING EXPERIMENT: A4_Transformer_Only =========================\n",
      "[2025-07-14 07:32:26] Resuming A4_Transformer_Only from checkpoint: checkpoint_epoch_009.pth\n",
      "[2025-07-14 07:32:26] Resumed from epoch 9. Best Val Loss so far: 0.5827\n",
      "[2025-07-14 07:32:26] A4_Transformer_Only training already completed. Loading best model for evaluation.\n",
      "[2025-07-14 07:32:26] --- Evaluating best A4_Transformer_Only model on Test Set ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de091c64bf74dd08da55c7f72785984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval A4_Transformer_Only:   0%|          | 0/376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-14 07:56:19] --- A4_Transformer_Only Test Results: ACC=0.7124, F1=0.6850, AUC=0.7815 ---\n",
      "[2025-07-14 07:56:19] Saved results for A4_Transformer_Only to results_ablation_studies/A4_Transformer_Only/results_A4_Transformer_Only.csv\n",
      "[2025-07-14 07:56:19] \n",
      "========================= STARTING EXPERIMENT: A5_Priors_Only =========================\n",
      "[2025-07-14 07:56:19] Resuming A5_Priors_Only from checkpoint: checkpoint_epoch_009.pth\n",
      "[2025-07-14 07:56:19] Resumed from epoch 9. Best Val Loss so far: 0.0584\n",
      "[2025-07-14 07:56:19] A5_Priors_Only training already completed. Loading best model for evaluation.\n",
      "[2025-07-14 07:56:19] --- Evaluating best A5_Priors_Only model on Test Set ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa37c576d2cd4b5dae039ff288c60151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval A5_Priors_Only:   0%|          | 0/376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-14 07:56:19] --- A5_Priors_Only Test Results: ACC=0.9767, F1=0.9763, AUC=0.9970 ---\n",
      "[2025-07-14 07:56:19] Saved results for A5_Priors_Only to results_ablation_studies/A5_Priors_Only/results_A5_Priors_Only.csv\n",
      "[2025-07-14 07:56:19] \n",
      "========================= STARTING EXPERIMENT: A1_Transformer_Priors =========================\n",
      "[2025-07-14 07:56:19] Resuming A1_Transformer_Priors from checkpoint: checkpoint_epoch_009.pth\n",
      "[2025-07-14 07:56:19] Resumed from epoch 9. Best Val Loss so far: 0.1530\n",
      "[2025-07-14 07:56:19] A1_Transformer_Priors training already completed. Loading best model for evaluation.\n",
      "[2025-07-14 07:56:19] --- Evaluating best A1_Transformer_Priors model on Test Set ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7637f772661e49289fc213103a2d71e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval A1_Transformer_Priors:   0%|          | 0/376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-14 08:19:39] --- A1_Transformer_Priors Test Results: ACC=0.9419, F1=0.9421, AUC=0.9877 ---\n",
      "[2025-07-14 08:19:39] Saved results for A1_Transformer_Priors to results_ablation_studies/A1_Transformer_Priors/results_A1_Transformer_Priors.csv\n",
      "[2025-07-14 08:19:39] \n",
      "========================= STARTING EXPERIMENT: A2_GAT_Priors =========================\n",
      "[2025-07-14 08:19:39] Resuming A2_GAT_Priors from checkpoint: checkpoint_epoch_009.pth\n",
      "[2025-07-14 08:19:39] Resumed from epoch 9. Best Val Loss so far: 0.1897\n",
      "[2025-07-14 08:19:39] A2_GAT_Priors training already completed. Loading best model for evaluation.\n",
      "[2025-07-14 08:19:39] --- Evaluating best A2_GAT_Priors model on Test Set ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89d71e090894f7e82b5d6879c8b8f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval A2_GAT_Priors:   0%|          | 0/376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-14 08:19:39] --- A2_GAT_Priors Test Results: ACC=0.9404, F1=0.9369, AUC=0.9850 ---\n",
      "[2025-07-14 08:19:39] Saved results for A2_GAT_Priors to results_ablation_studies/A2_GAT_Priors/results_A2_GAT_Priors.csv\n",
      "[2025-07-14 08:19:39] \n",
      "========================= STARTING EXPERIMENT: A3_Simple_Concat =========================\n",
      "[2025-07-14 08:19:39] Resuming A3_Simple_Concat from checkpoint: checkpoint_epoch_009.pth\n",
      "[2025-07-14 08:19:39] Resumed from epoch 9. Best Val Loss so far: 0.1600\n",
      "[2025-07-14 08:19:39] A3_Simple_Concat training already completed. Loading best model for evaluation.\n",
      "[2025-07-14 08:19:39] --- Evaluating best A3_Simple_Concat model on Test Set ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4337b7cfd754d848cde453fadff5d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval A3_Simple_Concat:   0%|          | 0/376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-14 08:42:50] --- A3_Simple_Concat Test Results: ACC=0.9371, F1=0.9391, AUC=0.9888 ---\n",
      "[2025-07-14 08:42:50] Saved results for A3_Simple_Concat to results_ablation_studies/A3_Simple_Concat/results_A3_Simple_Concat.csv\n",
      "[2025-07-14 08:42:50] \n",
      "--- All Ablation/Baseline Experiments Complete. Summary saved to results_ablation_studies/ablation_baselines_summary.csv ---\n",
      "\n",
      "--- Final Summary ---\n",
      "                       accuracy  f1_score       auc  precision    recall  \\\n",
      "model                                                                      \n",
      "A5_Priors_Only         0.976698  0.976255  0.996985   1.000000  0.953612   \n",
      "A3_Simple_Concat       0.937084  0.939130  0.988782   0.913534  0.966203   \n",
      "A1_Transformer_Priors  0.941911  0.942075  0.987659   0.943798  0.940358   \n",
      "A2_GAT_Priors          0.940413  0.936950  0.985045   1.000000  0.881378   \n",
      "A4_Transformer_Only    0.712383  0.685016  0.781548   0.761345  0.622598   \n",
      "\n",
      "                       specificity  \n",
      "model                               \n",
      "A5_Priors_Only            1.000000  \n",
      "A3_Simple_Concat          0.907692  \n",
      "A1_Transformer_Priors     0.943478  \n",
      "A2_GAT_Priors             1.000000  \n",
      "A4_Transformer_Only       0.803010  \n"
     ]
    }
   ],
   "source": [
    "# %% 5. Main Execution Block for Ablation & Baselines\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    log_message(\"--- Starting Ablation & Baseline Experiment Workflow ---\")\n",
    "    \n",
    "    class ArgsAblation: pass\n",
    "    args = ArgsAblation()\n",
    "    args.force_rerun = False # Set to True to retrain all models from scratch\n",
    "\n",
    "    try:\n",
    "        # --- Load & Prepare Data ONCE ---\n",
    "        log_message(\"\\n--- Step 1: Loading and Preparing Data for All Experiments ---\")\n",
    "        prom_seqs, prom_ids, prom_labels = load_sequences(PROMOTER_SEQ_FILE, True)\n",
    "        nonprom_seqs, nonprom_ids, nonprom_labels = load_sequences(NON_PROMOTER_SEQ_FILE, False)\n",
    "        all_seqs_raw=prom_seqs+nonprom_seqs; all_gene_ids=prom_ids+nonprom_ids; all_labels=prom_labels+nonprom_labels\n",
    "        master_gene_list=pd.Series(all_gene_ids).drop_duplicates().tolist()\n",
    "        priors_aligned, prior_dim = load_priors(PRIOR_FILE, master_gene_list)\n",
    "        gene_to_prior_map = {gid: priors_aligned[i] for i,gid in enumerate(master_gene_list)}\n",
    "        final_priors_for_ds = np.array([gene_to_prior_map.get(gid, np.zeros(prior_dim,dtype=np.float32)) for gid in all_gene_ids])\n",
    "        sequences_encoded = np.array([integer_encode_sequence(s) for s in tqdm(all_seqs_raw, desc=\"Encoding All Seqs\")])\n",
    "\n",
    "        # --- Load Precomputed GAT Output ---\n",
    "        main_bwaf_output_dir = 'results_bwaf_final/'\n",
    "        gat_output_path = os.path.join(main_bwaf_output_dir, \"precomputed_gat_output.pt\")\n",
    "        \n",
    "        if os.path.exists(gat_output_path):\n",
    "            log_message(f\"Loading pre-saved static GAT data from {gat_output_path}...\")\n",
    "            PRECOMPUTED_GAT_OUTPUT_STATIC = torch.load(gat_output_path, map_location='cpu') # Load to CPU\n",
    "        else:\n",
    "            log_message(f\"WARNING: Precomputed GAT output not found at {gat_output_path}. \"\n",
    "                        \"GAT-related ablations (A2, A3) will use a zero tensor.\")\n",
    "            num_genes_in_gat_data = len(master_gene_list)\n",
    "            PRECOMPUTED_GAT_OUTPUT_STATIC = torch.zeros(num_genes_in_gat_data, GAT_OUTPUT_DIM)\n",
    "\n",
    "        final_gat_gene_order = master_gene_list\n",
    "\n",
    "        # --- Create Full Dataset & Splits ONCE ---\n",
    "        full_dataset = MainDataset(sequences_encoded, all_gene_ids, all_labels,\n",
    "                                   final_priors_for_ds, PRECOMPUTED_GAT_OUTPUT_STATIC.numpy(), final_gat_gene_order)\n",
    "        indices = list(range(len(full_dataset))); np.random.seed(RANDOM_SEED); np.random.shuffle(indices)\n",
    "        test_idx = int(np.floor(TEST_SPLIT*len(full_dataset))); val_idx = test_idx + int(np.floor(VALIDATION_SPLIT*len(full_dataset)))\n",
    "        train_indices=indices[val_idx:]; val_indices=indices[test_idx:val_idx]; test_indices=indices[:test_idx]\n",
    "        train_ds=Subset(full_dataset,train_indices); val_ds=Subset(full_dataset,val_indices); test_ds=Subset(full_dataset,test_indices)\n",
    "        log_message(f\"Ablation Data Split: Train={len(train_ds)}, Val={len(val_ds)}, Test={len(test_ds)}\")\n",
    "        num_workers = 0; drop_last_train = (len(train_ds) % BATCH_SIZE == 1) and len(train_ds) > 1\n",
    "        train_loader=DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=num_workers, drop_last=drop_last_train)\n",
    "        val_loader=DataLoader(val_ds, BATCH_SIZE, shuffle=False, num_workers=num_workers)\n",
    "        test_loader=DataLoader(test_ds, BATCH_SIZE, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        # --- Define and Run Experiments ---\n",
    "        all_results = []\n",
    "        experiments_to_run = {\n",
    "            \"A4_Transformer_Only\": TransformerOnly(embed_dim=EMBEDDING_DIM, fusion_hidden_dim=FUSION_HIDDEN_DIM, dropout=DROPOUT_RATE),\n",
    "            \"A5_Priors_Only\": PriorsOnly(prior_dim=prior_dim, fusion_hidden_dim=FUSION_HIDDEN_DIM, dropout=DROPOUT_RATE),\n",
    "            \"A1_Transformer_Priors\": TransformerPriorsOnly(prior_dim=prior_dim, embed_dim=EMBEDDING_DIM, fusion_hidden_dim=FUSION_HIDDEN_DIM, dropout=DROPOUT_RATE),\n",
    "            \"A2_GAT_Priors\": GATPriorsOnly(prior_dim=prior_dim, gat_output_dim=GAT_OUTPUT_DIM, fusion_hidden_dim=FUSION_HIDDEN_DIM, dropout=DROPOUT_RATE),\n",
    "            \"A3_Simple_Concat\": SimpleConcatModel(prior_dim=prior_dim, embed_dim=EMBEDDING_DIM, gat_output_dim=GAT_OUTPUT_DIM, fusion_hidden_dim=FUSION_HIDDEN_DIM, dropout=DROPOUT_RATE)\n",
    "        }\n",
    "\n",
    "        for name, model in experiments_to_run.items():\n",
    "            model_results_path = os.path.join(OUTPUT_DIR_ABLATION, f\"results_{name}.csv\")\n",
    "            if os.path.exists(model_results_path) and not args.force_rerun:\n",
    "                log_message(f\"\\nResults for {name} already exist. Skipping experiment.\")\n",
    "                try:\n",
    "                    all_results.append(pd.read_csv(model_results_path).to_dict('records')[0])\n",
    "                except pd.errors.EmptyDataError:\n",
    "                    log_message(f\"Warning: Found empty results file for {name}. Rerunning.\")\n",
    "                    # Fall through to run the experiment\n",
    "                except Exception as e:\n",
    "                    log_message(f\"Warning: Could not read existing results for {name}: {e}. Rerunning.\")\n",
    "                else:\n",
    "                    continue # Skip to next model\n",
    "\n",
    "            results = run_experiment(name, model, train_loader, val_loader, test_loader,\n",
    "                                     NUM_EPOCHS, LEARNING_RATE, DEVICE, OUTPUT_DIR_ABLATION,\n",
    "                                     precomputed_gat_output=PRECOMPUTED_GAT_OUTPUT_STATIC)\n",
    "            if results: all_results.append(results)\n",
    "\n",
    "        # --- Summarize All Results ---\n",
    "        if all_results:\n",
    "            summary_df = pd.DataFrame(all_results)\n",
    "            # Check if 'model' column exists before setting index\n",
    "            if 'model' in summary_df.columns:\n",
    "                summary_df = summary_df.set_index('model')\n",
    "            else:\n",
    "                log_message(\"Warning: 'model' column not found in results list, cannot set as index.\")\n",
    "            \n",
    "            summary_path = os.path.join(OUTPUT_DIR_ABLATION, \"ablation_baselines_summary.csv\")\n",
    "            summary_df.to_csv(summary_path)\n",
    "            log_message(f\"\\n--- All Ablation/Baseline Experiments Complete. Summary saved to {summary_path} ---\")\n",
    "            \n",
    "            print(\"\\n--- Final Summary ---\")\n",
    "           \n",
    "            if 'auc' in summary_df.columns:\n",
    "                print(summary_df.sort_values(by='auc', ascending=False))\n",
    "            else:\n",
    "                log_message(\"Warning: 'auc' column not found in final summary DataFrame. Cannot sort.\")\n",
    "                print(summary_df)\n",
    "        else:\n",
    "            log_message(\"--- No experiments completed successfully. ---\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log_message(f\"--- ABLATION/BASELINE WORKFLOW FAILED ---\");\n",
    "        log_message(f\"ERROR: {type(e).__name__}: {e}\");\n",
    "        log_message(\"Traceback:\\n\" + traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f70b6-64a1-4145-ac36-fe8edc3a8969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b30865-ef7c-4426-a66f-5f176fddd617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2170e2-d49f-4697-a07f-a55f3f12411a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56fbf8-114e-4d81-b746-aae22b87ea6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3597c-3da5-4fae-bde9-a2756430f6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac797f-ca87-430a-85be-f7b722c16a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c2d8de-fa28-4c26-8453-7baacd1ce9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb581a7-6699-4dee-93f3-feba0b96c19c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d22a8-3440-43ed-8c0a-526aa10a2111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
